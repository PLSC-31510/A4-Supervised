---
title: "Assignment 4"
author: "PLSC 21510/31510"
date: "2022"
output: html_document
---

Assigned: May 5, 2022
Due: May 18, 2022

# Part 1. Hilary's Emails

In this section we will begin analyzing a collection of emails Hillary Clinton released as part of her (potentially improper) use of a private email server. 

The dataset that we use comes from Kaggle where a team processed an initial release of 7,946 emails to create an easy to read csv file. The complete download is available here:
https://www.kaggle.com/kaggle/hillary-clinton-emails.

We’re going to work with the `Emails.csv` file, which is available in the directory on coursework for this assignment. Run the following code to preprocess the data:

```{r}
library(tidyverse)
library(glmnet)
library(quanteda)
library(quanteda.textstats)
library(tidytext)

# read csv
clinton <- read.csv("emails.csv") 

# tokenize
clinton.toks <- clinton %>% 
  corpus(text_field = "RawText", docid_field = "Id") %>%
  tokens(split_hyphens = TRUE,
         remove_punct = TRUE,
         remove_numbers = TRUE,
         remove_symbols = TRUE,
         remove_url = TRUE) %>%
  tokens_tolower()  

# make dtm
clinton.dtm <- clinton.toks %>%
  tokens_remove(pattern = stopwords("en")) %>%
  dfm()
```

## 1. Benghazi

You might recall that there was considerable controversy over Hillary Clinton’s role in an incident in Benghazi where a US ambassador and other foreign service officers were killed. We're going to count the number of times Benghazi is used and how it is used in her emails.

### 1.1 

Count the number of times "Benghazi" is used in each email. Print the ID of the email(s) with the highest frequency.

```{r}
# YOUR CODE HERE
```

### 1.2 

Using KWIC, find the 5 words before and after "benghazi" in the emails. Based on your impressions (and not a quantitative analysis, unless you want), when do mentions of Benghazi tend to occur in her email?

```{r}
# YOUR CODE HERE
```

## 2. Sentiment

### 2.1

Using the `bing` dictionary from the `tidytext::get_sentiments` function, calculate the positive sentiment (as a proportion of all pos+neg words) for each email. Print the sentiments of the first 5 emails.

```{r}
# YOUR CODE HERE
```

### 2.2

Regress the positive sentiment score against the number of times Benghazi is mentioned in an email. What do you notice about the relationship?

```{r}
# YOUR CODE HERE
```

### 2.3

Another sentiment dictionary is the 2015 Lexicoder Sentiment Dictionary, which is available in the `data_dictionary_LSD2015` object from quanteda. The dictionary contains both negative/positive as well as "neg_positive" and "neg_negative" phrases 

Read the documentation first! Then, use the dictionary to recalculate the sentiment of the emails. Both "negative" and "neg_positive" frequencies should be counted as "negative, and vice versa for "postive and "neg_negative". 

After you recalculate the positive sentiment score, re-estimate its relationship to "Benghazi" frequency. Did your results change?

```{r}
# YOUR CODE HERE
```


# Part 2: Credit Claiming in Congressional Texts

In *The Impression of Influence*, Grimmer, Westwood, and Messing analyze the rate members of Congress claim credit for government spending in their press releases. Members of Congress issue a lot of press releases: from 2005 to 2010, House members issues nearly *170,000* press releases.

Given that it would be hard to analyze such a large collection by hand, GWM decided to use supervised learning methods. They hired a team of Stanford undergraduates to classify a random sample of 800 press releases as "credit claiming" or not. 

The object `CreditClaim.RData` contains the list `credit_claim`. The first element of this list (named `x`) is the *document term matrix* (already preprocessed for you) and the second element (`y`) are the labels.

Run the code below to get started.

```{r}
# Load `CreditClaim.RData` into R.
load("CreditClaim.RData")
dtm <- as.data.frame(credit_claim$x)
dtm$y <- credit_claim$y
```

## 3. Logistic vs. Lasso

### 3.1

Using a *logistic* regression, predict the credit claiming labels using all features. What warning message do you receive and what do you notice about the coefficients? (warning: this might take awhile)

```{r}
# YOUR CODE HERE
```

### 3.2

Using the `glmnet` library, fit a LASSO regression (*logistic* model). Plot the number of non-zero coefficients at different values of λ. What do you notice?

```{r}
# YOUR CODE HERE
```

## 4. In-sample accuracy.

### 4.1 

Write a function called `misclassification` that takes two arguments `predict` and `true` (both numeric vectors of 0's or 1's), and returns the misclassification error (i.e., 1 - accuracy)

```{r}
# YOUR CODE HERE

# uncomment to test -- should return 0.3333333
# misclassification(c(0, 0, 1), c(0, 1, 1))
```

### 4.2

Plot the in-sample misclassification error at different values of λ. 

**Hint**: Use the `type = "class"` argument in `predict` to get the predicted class label. Make sure to convert to numeric before passing into your `misclassification` function.

```{r}
# YOUR CODE HERE
``` 

### 4.3

What value of λ provides the lowest in-sample misclassification rate? Print the number of non-zero coefficients for that model.

```{r}
# YOUR CODE HERE
```

## 5. Cross Validation

### 5.1

Perform a 10-fold cross validation for the LASSO model, calculating the misclassification error for each value of λ. 

Plot the out-of-sample error for each value of λ.

**Hint**: The parameter `type.measure = "class` in `cv.glmnet()` will calculate the misclassification error for you.

[NB: This might take awhile in computing time.]

```{r}
# set seed for random sampling
set.seed(411)

# YOUR CODE HERE
```

### 5.2

What value of λ provides the lowest out-of-sample error? How does the out-of-sample error compare to the optimal in-sample error from the previous question? How many non-zero predictors are in this model?

```{r}
# YOUR CODE HERE
```